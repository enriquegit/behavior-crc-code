idx <- which(trajectories$label == "abnormal")
length(unique(trajectories$id[idx]))
trj.normal <- trajectories[-idx,]
trj.abnormal <- trajectories[idx,]
# Select 50 normal trajectories.
sel.normal <- head(unique(trj.normal$id),50)
# Select 10 abnormal trajectories.
sel.abnormal <- unique(trj.abnormal$id)[0:10]
trj.selected <- trajectories[which(trajectories$id %in% c(sel.normal, sel.abnormal)), ]
library(anipaths)
animate_paths(paths = trj.selected,
n.frames = max(table(trj.selected$id)),
interval = 1/2,
plot.date = F,
tail.wd = 0.2,
tail.length = 100,
pt.cex = 1.5,
pt.colors = "white",
legend.loc = NA,
tail.colors = c(rep("blue",length(sel.normal)), rep("red",length(sel.abnormal))),
covariate = "label",
covariate.colors = c("blue","red"),
coord = c("x.coord", "y.coord"),
Time.name = "times",
ID.name = "id")
source(file.path("..","auxiliary_functions","globals.R"))
source(file.path("..","auxiliary_functions","functions.R"))
library(R.matlab)
library(trajr)
# Read the database file.
df <- readMat(file.path(datasets_path,
"fish_trajectories","fishDetections_total3102.mat"))$fish.detections
n <- dim(df)[3] # Number of trajectories.
minFrames <- 10 # Only trajectories with a minimum number of frames will be considered.
total <- NULL # data frame to save features.
for(i in 1:n){
if(i %% 100)print(paste0("Processing trajectory ", i, "/", n))
trj <- df[,,i]
if(length(trj$frame.number) < minFrames)next;
# Compute center of bounding box.
x.coord <- trj$bounding.box.x + (trj$bounding.box.w / 2)
y.coord <- trj$bounding.box.y + (trj$bounding.box.h / 2)
times <- trj$frame.number - trj$frame.number[1] # Make times start at 0.
tmp <- data.frame(x.coord, y.coord, time=times)
tmp.trj <- TrajFromCoords(tmp, fps = 1)
#png("traj_resampled_plot.png", width = 6, height = 4, units = "in", res = 200)
#par(mar=c(5,5,2,2))
#plot(tmp.trj, lwd = 1, xlab="x", ylab="y")
#points(tmp.trj, draw.start.pt = T, pch = 1, col = "blue", cex = 1.2)
resampled <- TrajResampleTime(tmp.trj, 1)
#points(resampled, pch = 4, col = "red", cex = 0.8)
#legend("topright", c("Starting point"), pch = c(16), col=c("black"))
#legend("topright", c("Starting point","Original trajectory","Resampled trajectory"), pch = c(16,1,4), col=c("black","blue","red"))
#dev.off()
derivs <- TrajDerivatives(resampled)
#head(derivs$speed)
head(derivs$acceleration)
f.meanSpeed <- mean(derivs$speed)
f.sdSpeed <- sd(derivs$speed)
f.minSpeed <- min(derivs$speed)
f.maxSpeed <- max(derivs$speed)
f.meanAcc <- mean(derivs$acceleration)
f.sdAcc <- sd(derivs$acceleration)
f.minAcc <- min(derivs$acceleration)
f.maxAcc <- max(derivs$acceleration)
features <- data.frame(id=paste0("id",i), label=trj$classDescription[1],
f.meanSpeed, f.sdSpeed, f.minSpeed, f.maxSpeed,
f.meanAcc, f.sdAcc, f.minAcc, f.maxAcc)
total <- rbind(total, features)
}
# Read dataset.
dataset <- read.csv(file.path(datasets_path,
"fish_trajectories","fishFeatures.csv"),
stringsAsFactors = T)
# Print first rows of the dataset.
head(dataset)
table(dataset$label)
dataset <- normalize(dataset, dataset)$train
summary(dataset)
labels <- unique(dataset$label)
cols <- as.integer(dataset$label) + 1
d <- dist(dataset[,3:ncol(dataset)])
fit <- cmdscale(d, k = 2) # k is the number of dimensions.
x <- fit[,1]; y <- fit[,2]
#png("mdsFishes.png", width = 5, height = 4, units = "in", res = 720)
plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2", main="MDS trajectories features", pch=19, col=cols, cex=0.7)
legend("topleft", legend = labels, pch=19, col=unique(cols), cex=0.7, horiz = F)
# Create some random data.
set.seed(123)
n <- 50
x <- rnorm(n)
par(mar=c(2,0.5,0.5,0.5))
plot(x,rep(1,n), yaxt = 'n', ylab = "", col="gray", pch=1, cex=1.0, ylim = c(1,1.1))
# Make the max be the anomaly.
xanomaly <- max(x)
points(xanomaly,1, col="red", pch = 17, cex=1.1)
# Select another point to be the normal one.
xnormal <- x[10]
points(xnormal,1, col="blue", pch = 19, cex=1.1)
# Add a legend.
legend("topleft", c("anomaly","normal"),
pch = c(17,19),
col=c("red","blue"))
# Recursive function to isolate a selected point x.
isolate.point <- function(x, point, res){
# The point of interest is now isolated.
if(length(x)==1 & x[1] == point){
return(list(h=0, splitpoint=res$splitpoint))
}
# The point of interest was not found in this path.
if(length(x)==1)return(NULL)
# Select a random split point.
sp <- runif(1, min=min(x), max=max(x))
#abline(v=sp)
lx <- x[which(x <= sp)]
rx <- x[which(x > sp)]
lres <- isolate.point(lx, point, list(h=res$h, splitpoint=sp))
rres <- isolate.point(rx, point, list(h=res$h, splitpoint=sp))
if(is.null(lres) && is.null(rres))return(NULL)
if(is.null(lres)){
return(list(h=1+rres$h, splitpoint=c(res$splitpoint, rres$splitpoint)))
}
else {
return(list(h=1+lres$h, splitpoint=c(res$splitpoint, lres$splitpoint)))
}
}
res1 <- isolate.point(x, xanomaly, list(h=0, splitpoint=NULL))
abline(v=res1$splitpoint, col="red")
res2 <- isolate.point(x, xnormal, list(h=0, splitpoint=NULL))
abline(v=res2$splitpoint, col="blue")
#### Construct several trees ####
isolate.point.iter <- function(x, point, niter=10){
avg.h <- NULL
for(i in 1:niter){
tmp.h <- 0
for(j in 1:i){
res <- isolate.point(x, point, list(h=0, splitpoint=NULL))
tmp.h <- tmp.h + res$h
}
avg.h <- c(avg.h, tmp.h / i)
}
return(avg.h)
}
set.seed(123)
heights.anomaly <- isolate.point.iter(x, xanomaly, niter=200)
heights.normal <- isolate.point.iter(x, xnormal, niter=200)
plot(heights.anomaly, type="l", ylim = c(2.0,10), col="red",
main = "Average path lengths",
ylab = "average path length",
xlab = "# trees")
lines(heights.normal, col="blue", lty = 2)
legend("right", c("normal","anomaly"),
lty = c(2,1),
col=c("blue","red"))
tail(heights.normal)
tail(heights.anomaly)
# Create some random data.
set.seed(1234)
n <- 4000; # Normal points.
n.anomalies <- 100 # Anomaly points.
x <- rnorm(n)
y <- rnorm(n)
ax1 <- rnorm(n.anomalies, mean = -2.7, sd = 0.1)
ay1 <- rnorm(n.anomalies, mean = 1.8, sd = 0.3)
df <- data.frame(x=c(x,ax1), y=c(y,ay1), type=c(rep("normal",n), rep("anomaly",n.anomalies)))
plot(df[df$type=="normal",1],df[df$type=="normal",2], col="gray", pch=1, cex=1.0, main = "Original dataset (4100 instances)", xlab = "x", ylab = "y", xlim = c(-3.5,3.5), ylim = c(-3.5,3.5))
points(df[df$type=="anomaly",1], df[df$type=="anomaly",2], col="red", pch=2)
legend("topright", c("anomaly","normal"),
pch = c(17,1),
col=c("red","gray"))
p <- 256 # Sampling size.
reduced.df <- df[sample(nrow(df), size = p, replace = F),]
plot(reduced.df[reduced.df$type=="normal",1],reduced.df[reduced.df$type=="normal",2], col="gray", pch=1, cex=1.0, main = "After sampling (256 instances)", xlab = "x", ylab = "y", xlim = c(-3.5,3.5), ylim = c(-3.5,3.5))
points(reduced.df[reduced.df$type=="anomaly",1], reduced.df[reduced.df$type=="anomaly",2], col="red", pch=2)
legend("topright", c("anomaly","normal"),
pch = c(17,1),
col=c("red","gray"))
source(file.path("..","auxiliary_functions","globals.R"))
source(file.path("..","auxiliary_functions","functions.R"))
library(solitude)
library(PRROC)
# Read dataset. This one contains the extracted features from the raw fish trajectories.
dataset <- read.csv(file.path(datasets_path,
"fish_trajectories","fishFeatures.csv"),
stringsAsFactors = T)
# We will try without abnormal cases during training. We will do the same in the autoencoder example.
# All the abnormal cases go to the test set.
test.abnormal <- dataset[dataset$label == "abnormal",]
# Percent to use as train set.
pctTrain <- 0.80
# Corresponding number of samples.
nsamples <- ceiling(sum(dataset$label == "normal") * pctTrain)
# Generate the indices of the train set instances.
set.seed(123)
idxs <- sample(nsamples)
# Train set consisting of only normal instances.
train.normal <- dataset[dataset$label == "normal", ][idxs,]
# The test set of normal instances.
test.normal <- dataset[dataset$label == "normal", ][-idxs,]
# Build a test set with both, normal and abnormal points.
test.all <- rbind(test.normal, test.abnormal)
# First we define the parameters. nproc is the numbers of cores in the CPU.
# I set it to 1 to get reproducible results.
m.iforest <- isolationForest$new(sample_size = 256,
num_trees = 100,
nproc = 1)
# Fit the model with the train data and without ids and labels.
m.iforest$fit(train.normal[,-c(1:2)])
# Predict anomaly scores on train set.
train.scores <- m.iforest$predict(train.normal[,-c(1:2)])
# Print first rows of predictions.
head(train.scores)
# Sort and display instances with the highest anomaly scores.
head(train.scores[order(anomaly_score, decreasing = TRUE)])
hist(train.scores$anomaly_score)
quantile(train.scores$anomaly_score)
threshold <- 0.7603
# Predict anomaly scores on test set.
test.scores <- m.iforest$predict(test.all[,-c(1:2)])
# Predict labels based on threshold.
predicted.labels <- as.integer((test.scores$anomaly_score > threshold))
# Count how many abnormal instances were detected (not necessarily true positives).
sum(predicted.labels)
# All abnormal cases are at the end so we can compute the ground truth as follows.
gt.all <- c(rep(0,nrow(test.normal)), rep(1, nrow(test.abnormal)))
levels <- c("0","1")
cm <- confusionMatrix(factor(predicted.labels, levels = levels),
factor(gt.all, levels = levels),
positive = "1")
# Print confusion matrix.
cm$table
# Print sensitivity
cm$byClass["Sensitivity"]
roc_obj <- roc.curve(scores.class0 = test.scores$anomaly_score,
weights.class0 = gt.all, curve = TRUE, rand.compute = T)
# Print first values of the curve table.
head(roc_obj$curve)
#png("roc_curve.png", width = 6, height = 4, units = "in", res = 720)
plot(roc_obj, rand.plot = T)
# Print first values of the curve table.
head(roc_obj$curve)
source(file.path("..","auxiliary_functions","globals.R"))
source(file.path("..","auxiliary_functions","functions.R"))
library(keras)
library(PRROC)
# Read dataset. This one contains the extracted features from the raw fish trajectories.
dataset <- read.csv(file.path(datasets_path,
"fish_trajectories","fishFeatures.csv"),
stringsAsFactors = T)
# Since this is unsupervised, we do not need the abnormal cases during training,
# so all abnormal cases are used for testing.
test.abnormal <- dataset[dataset$label == "abnormal",]
# Percent to use as train set.
pctTrain <- 0.80
# Corresponding number of samples.
nsamples <- ceiling(sum(dataset$label == "normal") * pctTrain)
# Generate the indices of the train set instances.
set.seed(123)
idxs <- sample(nsamples)
train.normal <- dataset[dataset$label == "normal", ][idxs,]
test.normal <- dataset[dataset$label == "normal", ][-idxs,]
# Define function to standardize the data.
normalize.standard <- function(data, means = NA, sds = NA){
# Define variables to store the learned parameters. (mean and sd).
paramMeans <- NULL
paramSds <- NULL
# Iterate columns
for(i in 1:ncol(data)){
c <- data[,i]
if(is.na(means) || is.na(sds)){
tmpMean <- mean(c)
tmpSd <- sd(c)
paramMeans <- c(paramMeans, tmpMean)
paramSds <- c(paramSds, tmpSd)
}
else{
tmpMean <- means[i]
tmpSd <- sds[i]
}
normalizedCol <- (c - tmpMean) / tmpSd
data[,i] <- normalizedCol
}
return(list(data=data, means=paramMeans, sds=paramSds))
}
# Use our previously defined function to normalize the data.
res <- normalize.standard(train.normal[,-c(1:2)])
train.normal <- cbind(train.normal[,1:2], res$data)
# Normalize the test data using the learned parameters.
test.normal <- cbind(test.normal[,1:2],
normalize.standard(test.normal[,-c(1:2)],
res$means, res$sds)$data)
test.abnormal <- cbind(test.abnormal[,1:2],
normalize.standard(test.abnormal[,-c(1:2)],
res$means,
res$sds)$data)
#### Define Autoenconder ####
autoencoder <- keras_model_sequential()
autoencoder %>%
layer_dense(units = 32, activation = 'relu', input_shape = ncol(train.normal)-2) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 8, activation = 'relu') %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 32, activation = 'relu') %>%
layer_dense(units = ncol(train.normal)-2, activation = 'linear')
# Print a summary of the autoencoder.
summary(autoencoder)
autoencoder %>% compile(
loss = 'mse',
optimizer = optimizer_sgd(lr = 0.01),
metrics = c('mse')
)
history <- autoencoder %>% fit(
as.matrix(train.normal[,-c(1:2)]),
as.matrix(train.normal[,-c(1:2)]),
epochs = 100,
batch_size = 32,
validation_split = 0.10,
verbose = 2,
view_metrics = TRUE
)
plot(history)
# Compute MSE of normal test set.
autoencoder %>% evaluate(as.matrix(test.normal[,-c(1:2)]),
as.matrix(test.normal[,-c(1:2)]))
# Compute MSE of normal test set.
autoencoder %>% evaluate(as.matrix(test.normal[,-c(1:2)]),
as.matrix(test.normal[,-c(1:2)]))
# Compute MSE of abnormal test set.
autoencoder %>% evaluate(as.matrix(test.abnormal[,-c(1:2)]),
as.matrix(test.abnormal[,-c(1:2)]))
# This function computes the squared errors for each instance.
# The total error for an instance is the sum of the squared errors of each feature.
squared.errors <- function(preds, groundTruth){
squaredErrors <- NULL
for(i in 1:nrow(preds)){
se <- sum((preds[i,] - groundTruth[i,])^2)
squaredErrors <- c(squaredErrors, se)
}
return(squaredErrors)
}
# Predict values on the normal train set.
preds.train.normal <- autoencoder %>%
predict_on_batch(as.matrix(train.normal[,-c(1:2)]))
# Compute individual reconstruction errors in train set.
errors.train.normal <- squared.errors(preds.train.normal, as.matrix(train.normal[,-c(1:2)]))
hist(errors.train.normal)
mean(errors.train.normal)
quantile(errors.train.normal)
threshold <- 1.0
# Make predictions on the abnormal test set.
preds.test.abnormal <- autoencoder %>% predict_on_batch(as.matrix(test.abnormal[,-c(1:2)]))
# Compute reconstruction errors.
errors.test.abnormal <- squared.errors(preds.test.abnormal, as.matrix(test.abnormal[,-c(1:2)]))
# Predict labels based on threshold 1:abnormal, 0:normal.
pred.labels.abnormal <- as.integer((errors.test.abnormal > threshold))
# Count how many abnormal instances were detected.
sum(pred.labels.abnormal)
# Make predictions on the normal test set.
preds.test.normal <- autoencoder %>% predict_on_batch(as.matrix(test.normal[,-c(1:2)]))
# Compute reconstruction errors.
errors.test.normal <- squared.errors(preds.test.normal, as.matrix(test.normal[,-c(1:2)]))
# Predict labels based on threshold 1:abnormal, 0:normal.
pred.labels.normal <- as.integer((errors.test.normal > threshold))
# Count how many normal instances were misclassified as abnormal (false positives).
sum(pred.labels.normal)
# Compute some metrics.
pred.labels.all <- as.factor(c(pred.labels.normal, pred.labels.abnormal))
gt.all <- c(rep(0,nrow(test.normal)), rep(1,nrow(test.abnormal)))
cm <- confusionMatrix(pred.labels.all, as.factor(gt.all), positive = "1")
# Print confusion matrix.
cm$table
# Print sensitivity
cm$byClass["Sensitivity"]
# Generate ROC curve with PRROC package.
scores.all <- c(errors.test.normal, errors.test.abnormal)
roc_obj <- roc.curve(scores.class0 = scores.all,
weights.class0 = gt.all, curve = TRUE)
#png("roc_curve_autoencoder.png", width = 6, height = 4, units = "in", res = 720)
plot(roc_obj)
source(file.path("..","..","auxiliary_functions","globals.R"))
library(keras)
library(caret)
setwd("C:/Users/yo/Google Drive/my_works/my_books/behavior/code-crc/Predicting Behavior with Deep Learning/keras")
source(file.path("..","..","auxiliary_functions","globals.R"))
library(keras)
library(caret)
# Path to dataset directory.
pathdir <- file.path(datasets_path,
"electromyography/")
# Read dataset.
dataset <- NULL
# Each class is saved in a different file so we append all.
for(i in 0:3){
tmp <- read.csv(file.path(pathdir,paste0(i,".csv")), header = F)
dataset <- rbind(dataset,tmp)
}
# Check the class distribution.
table(dataset$V65)
# Shuffle rows.
set.seed(1234)
dataset <- dataset[sample(nrow(dataset), size = nrow(dataset)),]
# Split data. train 60%, validation 10%, test 30%.
group <- sample(1:3, size = nrow(dataset),
replace = TRUE,
prob = c(0.6, 0.10, 0.30))
trainset <- dataset[which(group == 1),]
valset <- dataset[which(group == 2),]
testset <- dataset[which(group == 3),]
# Write a function to normalize the variables.
normalize <- function(trainset, valset, testset){
# Function to normalize the train, validation and test set
# based on the parameters learned from the trainset.
# Iterate columns except the last one which is the class.
for(i in 1:(ncol(trainset)-1)){
c1 <- trainset[,i] # trainset column
c2 <- valset[,i] # valset column
c3 <- testset[,i] # testset column
max <- max(c1, na.rm = T) # Learn the max value from the trainset's column.
min <- min(c1, na.rm = T) # Learn the min value from the trainset's column.
if(max==min){ # If all values are the same set it to max.
trainset[,i] <- max
valset[,i] <- max
testset[,i] <- max
}
else{
# Normalize trainset's column.
trainset[,i] <- (c1 - min) / (max - min)
# Truncate max values in validation and test set.
idxs <- which(c2 > max)
if(length(idxs) > 0){
c2[idxs] <- max
}
idxs <- which(c3 > max)
if(length(idxs) > 0){
c3[idxs] <- max
}
# Truncate min values in validation and test set.
idxs <- which(c2 < min)
if(length(idxs) > 0){
c2[idxs] <- min
}
idxs <- which(c3 < min)
if(length(idxs) > 0){
c3[idxs] <- min
}
# Normalize validation and test set column.
valset[,i] <- (c2 - min) / (max - min)
testset[,i] <- (c3 - min) / (max - min)
}
}
return(list(train=trainset, val=valset, test=testset))
}
# Normalize data.
res <- normalize(trainset, valset, testset)
trainset <- res$train
valset <- res$val
testset <- res$test
# Define a function to format features and one-hot encode the class.
format.to.array <- function(data, numclasses = 4){
x <- as.matrix(data[, 1:(ncol(data)-1)])
y <- as.array(data[, ncol(data)])
y <- to_categorical(y, num_classes = numclasses)
l <- list(x=x, y=y)
return(l)
}
# Format data
trainset <- format.to.array(trainset, numclasses = 4)
valset <- format.to.array(valset, numclasses = 4)
testset <- format.to.array(testset, numclasses = 4)
# Display first classes from train set.
head(trainset$y)
# Define the network's architecture.
get.nn <- function(ninputs = 64, nclasses = 4, lr = 0.01){
model <- keras_model_sequential()
model %>%
layer_dense(units = 32, activation = 'relu', input_shape = ninputs) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = nclasses, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_sgd(lr = lr),
metrics = c('accuracy')
)
return(model)
}
model <- get.nn(64, 4, lr = 0.01)
summary(model)
history <- model %>% fit(
trainset$x, trainset$y,
epochs = 300,
batch_size = 8,
validation_data = list(valset$x, valset$y),
verbose = 1,
view_metrics = TRUE
)
plot(history)
# Evaluate model.
model %>% evaluate(testset$x, testset$y)
# Predict classes.
classes <- model %>% predict_classes(testset$x)
library(reticulate)
reticulate::virtualenv_remove(packages="h5py", envname = "r-reticulate")
source(file.path("..","..","auxiliary_functions","globals.R"))
library(keras)
library(caret)
reticulate::virtualenv_remove(packages="h5py", envname = "r-reticulate")
system2(config$python, c("-m", "pip", "install", "--quiet", shQuote("../input/h5py-legacy/h5py-2.10.0")))
config
source(file.path("..","..","auxiliary_functions","globals.R"))
library(keras)
library(caret)
model <- load_model_hdf5("electromyography.hdf5")
